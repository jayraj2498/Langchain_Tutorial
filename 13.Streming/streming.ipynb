{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# streming \n",
    "\n",
    "When the amount of data is come as output sometime it takes time to load \n",
    "\n",
    "By using streming we can remove that delay time , as we see if we give nay input to chatgpt they give \n",
    "us out without any further delay in the process  \n",
    "\n",
    "Streaming in LangChain refers to the capability of receiving responses from language models in real-time as they are generated, rather than waiting for the entire response to be produced before receiving it. This is particularly useful for applications where responsiveness is crucial, such as chatbots or live data processing. \n",
    "\n",
    "Faster User Feedback: Users can see responses being formed in real-time, making the interaction feel more dynamic and responsive.\n",
    "\n",
    "Resource Efficiency: It allows you to start processing the data sooner, which can be beneficial in time-sensitive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config \n",
    "secrete_key = config('OPENAI_API_KEY')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.callbacks.base import BaseCallbackHandler \n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "\n",
    "chat =ChatOpenAI(openai_api_key=secrete_key , streaming=True , callbacks=[BaseCallbackHandler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stremig 1 \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate , HumanMessagePromptTemplate \n",
    "\n",
    "chat_prompt= ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(\"tell me a fact about {topic}\")]) \n",
    "\n",
    "formatted_chat_prompt =chat_prompt.from_messages(topic=\"united states\") \n",
    "\n",
    "response =chat.invoke(formatted_chat_prompt)  \n",
    "\n",
    "print(response) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stremig 2  \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate , HumanMessagePromptTemplate \n",
    "\n",
    "chat_prompt= ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(\"tell me a fact about {topic}\")]) \n",
    "\n",
    "formatted_chat_prompt =chat_prompt.from_messages(topic=\"united states\") \n",
    "\n",
    "response =chat.invoke(formatted_chat_prompt)  \n",
    "\n",
    "for res in response :\n",
    "    print(res.content  , end=\" \" , flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_prompt= ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(\"tell me a fact about {topic}\")]) \n",
    "\n",
    "chain = chat_prompt | chat | StrOutputParser()\n",
    "\n",
    "response =chain.stream({\"topic\":\"pyton array \"} ) \n",
    "\n",
    "for res in response : \n",
    "    print(res.content , end=\" \" , flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template\n",
    "template = \"Translate the following English text to French: {text}\"\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "# Initialize the OpenAI language model with streaming enabled\n",
    "llm = OpenAI(api_key=\"your_openai_api_key\", stream=True)\n",
    "\n",
    "# Create a chain\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Function to handle streaming response\n",
    "def handle_streaming_response(response):\n",
    "    for chunk in response:\n",
    "        print(chunk, end='', flush=True)\n",
    "\n",
    "# Input text to translate\n",
    "input_text = {\"text\": \"Hello, how are you?\"}\n",
    "\n",
    "# Run the chain with streaming\n",
    "response = chain.run(input_text, handle_streaming_response=handle_streaming_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
