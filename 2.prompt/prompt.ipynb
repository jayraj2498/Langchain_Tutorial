{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     A prompt for language model is the set of instruction or input provide by user to guide model response \n",
    "### lang model : expect the prompt to either be string or list of chat messages \n",
    " \n",
    "to understand the context and generate relavent output such as Question answering , complting sentences , conversation \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prommpt templte are predefine for generating prompt for Lang model \n",
    "\n",
    "- template may include instruction , for specific context and Question for given task \n",
    "\n",
    "- lanchain provide tools to crete or work with prompts templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import HumanMessage \n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] template='tell me a python trick'\n",
      "input_variables=[] template='tell me a python trick'\n"
     ]
    }
   ],
   "source": [
    "no_input_prompt = PromptTemplate(input_variables=[] , template=\"tell me a python trick\")\n",
    "\n",
    "print(no_input_prompt)  \n",
    "\n",
    "# or \n",
    "\n",
    "no_input_prompt = PromptTemplate.from_template(\"tell me a python trick\")\n",
    "\n",
    "print(no_input_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me a python trick\n"
     ]
    }
   ],
   "source": [
    "no_input_prompt1 = PromptTemplate(input_variables=[] , template=\"tell me a python trick\")\n",
    "\n",
    "text =no_input_prompt1.format()  \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response \n",
      "\n",
      "One python trick is using list comprehension to create a new list based on the elements of an existing list. For example, if you have a list of numbers and you want to create a new list with only the even numbers, you can use list comprehension as follows:\n",
      "\n",
      "original_list = [1, 2, 3, 4, 5, 6]\n",
      "even_numbers = [x for x in original_list if x % 2 == 0]\n",
      "\n",
      "This will create a new list called \"even_numbers\" with only the even numbers from the original list. This can save time and make your code more concise.\n"
     ]
    }
   ],
   "source": [
    "# prompt hvae no input  \n",
    "\n",
    "llm= OpenAI(openai_api_key=OPENAI_API_KEY) \n",
    "\n",
    "no_input_prompt1 = PromptTemplate(input_variables=[] , template=\"tell me a python trick\")\n",
    "\n",
    "formated_no_ip_prompt =no_input_prompt1.format()   \n",
    "\n",
    "response=llm.invoke(formated_no_ip_prompt) \n",
    "\n",
    "print('response' ,response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response \n",
      "\n",
      "The top speed of Mahindra Thar varies depending on the engine and transmission options. The 2.2-liter diesel engine with a 6-speed manual transmission has a top speed of approximately 160 km/h (99 mph). The 2.0-liter petrol engine with a 6-speed automatic transmission has a top speed of approximately 160 km/h (99 mph) as well. However, the top speed may vary depending on the road and driving conditions. \n"
     ]
    }
   ],
   "source": [
    "# prompt have input or many input \n",
    "\n",
    "llm= OpenAI(openai_api_key=OPENAI_API_KEY) \n",
    "\n",
    "input_prompt = PromptTemplate(input_variables=[\"company\" ,\"car\"] , template=\"tell me {company} company {car} top spped \")\n",
    "\n",
    "formated_prompt =input_prompt.format(company=\"Mahindra\" , car=\"Thar\")  \n",
    "\n",
    "response=llm.invoke(formated_prompt)   \n",
    "\n",
    "print('response' ,response) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= OpenAI(open_api_key=OPENAI_API_KEY) \n",
    "\n",
    "inputs=PromptTemplate(input_variables=[\"language\" , \"topic\"] , template=\"tell me trick of {language} language of {topic} topic\")\n",
    "\n",
    "formated_input=inputs.format(language=\"python\" , topic=\"list\")  \n",
    "\n",
    "response=llm.invoke(formated_input)  \n",
    "\n",
    "print('response' ,response.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response \n",
      "\n",
      "\"Practice makes perfect, but a little trick can make it easier.\"\n"
     ]
    }
   ],
   "source": [
    "# OR \n",
    "# example 2  \n",
    "# prompt having one input  variable  \n",
    "\n",
    "\n",
    "one_input_prompt_1 =PromptTemplate.from_template(\n",
    "    \"tell me {language} trick in one line \") \n",
    "\n",
    "\n",
    "formated_1 =one_input_prompt_1.format(language=\"english\") \n",
    "\n",
    "response=llm.invoke(formated_1)   \n",
    "\n",
    "print('response' ,response) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response \n",
      "\n",
      "Why did Ironman go to the doctor?\n",
      "\n",
      "Because he had a case of the \"Iron-deficiencies\"!\n"
     ]
    }
   ],
   "source": [
    "# example 3 \n",
    "# prompt having two input  variable \n",
    "\n",
    "two_input_prompt_2=PromptTemplate.from_template( \n",
    "        \"tell me {adjective} joke about {content}\" ) \n",
    " \n",
    "formeted_prompt = two_input_prompt_2.format(adjective=\"funny\" , content =\"Ironman\")   \n",
    "\n",
    "response=llm.invoke(formeted_prompt) \n",
    "\n",
    "print('response' ,response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat model Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate  \n",
    "from langchain_core.prompts import HumanMessagePromptTemplate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatprompts is : input_variables=['input_lang', 'output_lang', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_lang', 'output_lang'], template='you are the helpfull assistant that translates {input_lang}  to {output_lang}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]\n"
     ]
    }
   ],
   "source": [
    "# chat model prompt tempates \n",
    "\n",
    "# ex 1 : message prompt templates as tuple   \n",
    "\n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages(  [\n",
    "    (\"system\" , \"you are the helpfull assistant that translates {input_lang}  to {output_lang}.\") , \n",
    "    (\"human\" , \"{text}\") , \n",
    "])  \n",
    "\n",
    "print('chatprompts is :' ,chatprompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so chatprompts consists of  \n",
    "- input_variables=['input_lang', 'output_lang', 'text'] \n",
    "\n",
    "- messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_lang', 'output_lang']\n",
    "\n",
    "- template='you are the helpfull assistant that translates {input_lang}  to {output_lang}. \n",
    "\n",
    "- HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_lang', 'output_lang', 'text']\n"
     ]
    }
   ],
   "source": [
    "# we can run it here  \n",
    "\n",
    "print(chatprompts.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formated_chart_prompt is : [SystemMessage(content='you are the helpfull assistant that translates english to hindi'), HumanMessage(content=' i am learning langchain from its website and from various other sources ')]\n"
     ]
    }
   ],
   "source": [
    "# chat model prompt tempates   \n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages(  [\n",
    "    (\"system\" , \"you are the helpfull assistant that translates {input_lang} to {output_lang}\") , \n",
    "    (\"human\" , \"{text}\") , \n",
    "])   \n",
    "\n",
    "\n",
    "formated_chart_prompt = chatprompts.format_messages(\n",
    "    input_lang=\"english\", \n",
    "    output_lang=\"hindi\" , \n",
    "    text = \" i am learning langchain from its website and from various other sources \"\n",
    ")\n",
    "\n",
    "print('formated_chart_prompt is :' ,formated_chart_prompt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemMessage: you are the helpfull assistant that translates english to hindi\n",
      "HumanMessage:  i am learning langchain from its website and from various other sources \n"
     ]
    }
   ],
   "source": [
    "# we can write in separte way \n",
    "\n",
    "system_message = formated_chart_prompt[0]\n",
    "print(\"SystemMessage:\", system_message.content)\n",
    "\n",
    "# Print the HumanMessage\n",
    "human_message = formated_chart_prompt[1]\n",
    "print(\"HumanMessage:\", human_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: मैं अपनी वेबसाइट और अन्य स्रोतों से 'लैंगचेन' सीख रहा हूं।\n"
     ]
    }
   ],
   "source": [
    "# chat model prompt tempates   \n",
    "\n",
    "llm= OpenAI(openai_api_key=OPENAI_API_KEY) \n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages(  [\n",
    "    (\"system\" , \"you are the helpfull assistant that translates {input_lang} to {output_lang}\") , \n",
    "    (\"human\" , \"{text}\") , \n",
    "])   \n",
    "\n",
    "\n",
    "formated_chart_prompt = chatprompts.format_messages(\n",
    "    input_lang=\"english\", \n",
    "    output_lang=\"hindi\" , \n",
    "    text = \" i am learning 'langchain' from its website and from various other sources \"\n",
    ")\n",
    "\n",
    "\n",
    "response=llm.invoke(formated_chart_prompt)  \n",
    "\n",
    "print(response)    # it will give you big output containing everything \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: Estoy aprendiendo Langchain de su sitio web y de varias otras fuentes.\n"
     ]
    }
   ],
   "source": [
    "# chat model prompt tempates   \n",
    "\n",
    "llm= OpenAI(openai_api_key=OPENAI_API_KEY) \n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages(  [\n",
    "    (\"system\" , \"you are the helpfull assistant that translates {input_lang} to {output_lang}\") , \n",
    "    (\"human\" , \"{text}\") , \n",
    "])   \n",
    "\n",
    "\n",
    "formated_chart_prompt = chatprompts.format_messages(\n",
    "    input_lang=\"english\", \n",
    "    output_lang=\"spanish\", \n",
    "    text = \" i am learning langchain from its website and from various other sources \"\n",
    ")\n",
    "\n",
    "\n",
    "response=llm.invoke(formated_chart_prompt)  \n",
    "\n",
    "print(response)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_lang', 'output_lang', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_lang', 'output_lang'], template='you are the helpfull assistant that translates {input_lang} to {output_lang}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text} '))]\n"
     ]
    }
   ],
   "source": [
    "# Ex : 2 -- using message classes\n",
    "\n",
    "sys_templates = \"you are the helpfull assistant that translates {input_lang} to {output_lang}\" \n",
    "human_templates = \"{text} \"\n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages([ \n",
    " SystemMessagePromptTemplate.from_template(sys_templates), \n",
    " HumanMessagePromptTemplate.from_template(human_templates)  \n",
    " ] )  \n",
    "\n",
    "print(chatprompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_lang', 'output_lang', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(chatprompts.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='you are the helpfull assistant that translates english to spanish'), HumanMessage(content=' i am learning langchain from its website and from various other sources  ')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sys_templates = \"you are the helpfull assistant that translates {input_lang} to {output_lang}\" \n",
    "human_templates = \"{text} \"\n",
    "\n",
    "chatprompts = ChatPromptTemplate.from_messages([ \n",
    " SystemMessagePromptTemplate.from_template(sys_templates), \n",
    " HumanMessagePromptTemplate.from_template(human_templates)  \n",
    " ] )  \n",
    "\n",
    "formated_chart_prompt = chatprompts.format_messages(\n",
    "    input_lang=\"english\", \n",
    "    output_lang=\"spanish\", \n",
    "    text = \" i am learning langchain from its website and from various other sources \"\n",
    ")\n",
    " \n",
    "\n",
    " \n",
    "# response =llm.invoke(formated_chart_prompt) \n",
    "# print(response)# run \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ip_lang', 'op_lang'] template='you are the help assitance that can translate {ip_lang} to {op_lang} '\n",
      "input_variables=['ip_lang', 'op_lang'] template='you are the help assitance that can translate {ip_lang} to {op_lang} '\n"
     ]
    }
   ],
   "source": [
    "## EX 3 \n",
    "## using prompt template   \n",
    "\n",
    "sys_prompt = PromptTemplate(\n",
    "    input_variables=[\"ip_lang\" , \"op_lang\"]  , \n",
    "    template=\"you are the help assitance that can translate {ip_lang} to {op_lang} \"  ) \n",
    "print(sys_prompt)  \n",
    "\n",
    "# or  \n",
    "\n",
    "sys_prompt = PromptTemplate.from_template(\"you are the help assitance that can translate {ip_lang} to {op_lang} \" )\n",
    "print(sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = PromptTemplate(\n",
    "    input_variables=[\"ip_lang\" , \"op_lang\"]  , \n",
    "    template=\"you are the help assitance that can translate {ip_lang} to {op_lang} \"  )   \n",
    "\n",
    "human_prompt = PromptTemplate.from_template(\"{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ip_lang', 'op_lang', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ip_lang', 'op_lang'], template='you are the help assitance that can translate {ip_lang} to {op_lang} ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = PromptTemplate(\n",
    "    input_variables=[\"ip_lang\" , \"op_lang\"]  , \n",
    "    template=\"you are the help assitance that can translate {ip_lang} to {op_lang} \"  )   \n",
    "\n",
    "human_prompt = PromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "sys_msg_prompt = SystemMessagePromptTemplate(prompt=sys_prompt)  \n",
    "human_msg_prompt = HumanMessagePromptTemplate(prompt=human_prompt)  \n",
    "\n",
    "chatprompt =ChatPromptTemplate.from_messages([sys_msg_prompt , human_msg_prompt]) \n",
    "\n",
    "print(chatprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ip_lang', 'op_lang', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(chatprompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System: That's great to hear! I can help you with translating English to German so you can communicate with people who speak that language. Is there something specific you would like to say?\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = PromptTemplate(\n",
    "    input_variables=[\"ip_lang\" , \"op_lang\"]  , \n",
    "    template=\"you are the help assitance that can translate {ip_lang} to {op_lang} \"  )   \n",
    "\n",
    "human_prompt = PromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "sys_msg_prompt = SystemMessagePromptTemplate(prompt=sys_prompt)  \n",
    "human_msg_prompt = HumanMessagePromptTemplate(prompt=human_prompt)  \n",
    "\n",
    "chatprompt =ChatPromptTemplate.from_messages([sys_msg_prompt , human_msg_prompt]) \n",
    "\n",
    "\n",
    "\n",
    "formated_chatprompt=chatprompt.format_messages(ip_lang = \"english\" ,\n",
    "                           op_lang= \"german\"  , \n",
    "                           text = \"i love langchain \")  \n",
    "\n",
    "\n",
    "\n",
    "response =llm.invoke(formated_chatprompt)  \n",
    "\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
