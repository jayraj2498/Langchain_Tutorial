{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you've loaded documents, you'll often want to transform them to better suit your application.\n",
    "### there is if the data is very big you cant fed  that to your model these is limit so, we give them by spliting  \n",
    "\n",
    "- there are various type if text spliter ; \n",
    "- recursive  , markdown , code , token , charecter , sementic chunk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text splitters in LangChain are powerful tools for dividing large texts into smaller, manageable chunks based on various criteria. Whether you're dealing with character limits, token constraints, or specific custom delimiters, LangChain provides flexible options to suit your needs. These splitters are essential for preprocessing text data for NLP tasks, making it easier to handle and process large documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main article: Stock exchange\n",
      "Interior hall of the Helsinki Stock Exchange in Helsinki, Finland, 1965\n",
      "A stock exchange is an exchange (or bourse) where stockbrokers and traders can buy and sell shares\n",
      "\n",
      "(equity stock), bonds, and other securities. Many large companies have their stocks listed on a stock exchange.\n",
      "This makes the stock more liquid and thus more attractive to many investors. The exchange may also act as a guarantor \n",
      "of settlement. These and other stocks may also be traded \"over the counter\" (OTC), that is, through a dealer\n",
      "\n",
      "Some large companies will have their stock listed on more than one exchange in different countries,\n",
      "so as to attract international investors.[4]\n",
      "Stock exchanges may also cover other types of securities, such as fixed-interest securities (bonds) or\n",
      " (less frequently) derivatives, which are more likely to be traded OTC.\n",
      "\n",
      "Trade in stock markets means the transfer (in exchange for money) of a stock or security from a seller to a buyer\n",
      "This requires these two parties to agree on a price. Equities (stocks or shares) confer an ownership interest in a \n",
      "particular company.\n",
      "\n",
      "Participants in the stock market range from small individual stock investors to larger investors, \n",
      "who can be based anywhere in the world, and may include banks, insurance companies, pension funds and hedge funds. \n",
      "Their buy or sell orders may be executed on their behalf by a stock exchange trader.\n",
      "\n",
      "Some exchanges are physical locations where transactions are carried out on a trading floor, \n",
      "by a method known as open outcry. This method is used in some stock exchanges and commodities exchanges, \n",
      "and involves traders shouting bid and offer prices. The other type of stock exchange has a network of \n",
      "computers where trades are made electronically. An example of such an exchange is the NASDAQ.\n",
      "\n",
      "A potential buyer bids a specific price for a stock, and a potential seller asks a specific price for the \n",
      "same stock. Buying or selling at the Market means you will accept any ask price or bid price for the stock. \n",
      "When the bid and ask prices match, a sale takes place, on a first-come, first-served basis \n",
      "if there are multiple bidders at a given price.\n",
      "\n",
      "The purpose of a stock exchange is to facilitate the exchange of securities between buyers and sellers, \n",
      "thus providing a marketplace. The exchanges provide real-time trading information on the listed securities, \n",
      "facilitating price discovery.\n",
      "\n",
      "The New York Stock Exchange (NYSE) is a physical exchange, with a hybrid market for placing orders electronically \n",
      "from any location as well as on the trading floor. Orders executed on the trading floor enter by way of exchange \n",
      "members and flow down to a floor broker, who submits the order electronically to the floor trading post for the\n",
      " Designated market maker (\"DMM\") for that stock to trade the order. The DMM's job is to maintain a two-sided market, \n",
      " making orders to buy and sell the security when there are no other buyers or sellers. If a bid–ask spread exists, \n",
      " no trade immediately takes place – in this case, the DMM may use their own resources (money or stock) to close the \n",
      " difference. Once a trade has been made, the details are reported on the \"tape\" and sent back to the brokerage firm, \n",
      " which then notifies the investor who placed the order. Computers play an important role, especially for program \n",
      " trading.\n",
      "\n",
      "The NASDAQ is an electronic exchange, where all of the trading is done over a computer network. \n",
      "The process is similar to the New York Stock Exchange. One or more NASDAQ market makers will always provide a bid and \n",
      "ask the price at which they will always purchase or sell 'their' stock.\n",
      "\n",
      "The Paris Bourse, now part of Euronext, is an order-driven, electronic stock exchange. \n",
      "It was automated in the late 1980s. Prior to the 1980s, it consisted of an open outcry exchange. \n",
      "Stockbrokers met on the trading floor of the Palais Brongniart. In 1986, the CATS trading system was introduced, \n",
      "and the order matching system was fully automated.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import   ChatOpenAI\n",
    "from langchain.text_splitter  import CharacterTextSplitter  \n",
    "\n",
    "#  split by charecter \n",
    "\n",
    "with open(\"E:\\\\Langchain_Prac\\\\6.text_splitter\\\\data.txt\") as f : \n",
    "    sample_data = f.read() \n",
    "    \n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 339, which is longer than the specified 300\n",
      "Created a chunk of size 319, which is longer than the specified 300\n",
      "Created a chunk of size 396, which is longer than the specified 300\n",
      "Created a chunk of size 355, which is longer than the specified 300\n",
      "Created a chunk of size 933, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Main article: Stock exchange\\nInterior hall of the Helsinki Stock Exchange in Helsinki, Finland, 1965\\nA stock exchange is an exchange (or bourse) where stockbrokers and traders can buy and sell shares'\n"
     ]
    }
   ],
   "source": [
    "#  split by charecter \n",
    "\n",
    "with open(\"E:\\\\Langchain_Prac\\\\6.text_splitter\\\\data.txt\") as f : \n",
    "    sample_data = f.read() \n",
    "    \n",
    "text_splitter =CharacterTextSplitter(separator=\"\\n\\n\"  , \n",
    "                                     chunk_size = 300) \n",
    "\n",
    "\n",
    "my_data =text_splitter.create_documents([sample_data])\n",
    "\n",
    "print(my_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some exchanges are physical locations where transactions are carried out on a trading floor, \n",
      "by a method known as open outcry. This method is used in some stock exchanges and commodities exchanges, \n",
      "and involves traders shouting bid and offer prices. The other type of stock exchange has a network of \n",
      "computers where trades are made electronically. An example of such an exchange is the NASDAQ.\n"
     ]
    }
   ],
   "source": [
    "print(my_data[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a long text document that needs to be split into smaller chunks.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"This is a long text document that needs to be split into smaller chunks.\"\n",
    "text_splitter = CharacterTextSplitter(chunk_size=20, chunk_overlap=0)\n",
    "docs = text_splitter.split_text(text) \n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split code splitter \n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (Language,RecursiveCharacterTextSplitter) \n",
    "\n",
    "# You can also see the separators used for a given language\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very long text document with multiple paragraphs and sentences. It covers a wide range of topics, including science, technology, history, and culture. The document is intended to provide a\n",
      "--------------------------------------------------------------------------------\n",
      "topics, including science, technology, history, and culture. The document is intended to provide a comprehensive overview of various subjects, making it an ideal resource for research and educational\n",
      "--------------------------------------------------------------------------------\n",
      "overview of various subjects, making it an ideal resource for research and educational purposes. However, due to its length, processing the entire document at once can be challenging for language\n",
      "--------------------------------------------------------------------------------\n",
      "However, due to its length, processing the entire document at once can be challenging for language models and other NLP tools. This is where text splitters like the RecursiveCharacterTextSplitter\n",
      "--------------------------------------------------------------------------------\n",
      "models and other NLP tools. This is where text splitters like the RecursiveCharacterTextSplitter come into play. By dividing the text into smaller, more manageable chunks, we can improve the\n",
      "--------------------------------------------------------------------------------\n",
      "come into play. By dividing the text into smaller, more manageable chunks, we can improve the efficiency and accuracy of our analysis and processing tasks. The RecursiveCharacterTextSplitter is\n",
      "--------------------------------------------------------------------------------\n",
      "efficiency and accuracy of our analysis and processing tasks. The RecursiveCharacterTextSplitter is particularly useful because it not only splits the text based on character length but also\n",
      "--------------------------------------------------------------------------------\n",
      "is particularly useful because it not only splits the text based on character length but also recursively splits the resulting chunks if they exceed a specified character limit. This ensures that the\n",
      "--------------------------------------------------------------------------------\n",
      "splits the resulting chunks if they exceed a specified character limit. This ensures that the chunks remain within the desired size range while preserving semantic coherence as much as possible.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "This is a very long text document with multiple paragraphs and sentences. It covers a wide range of topics, including science, technology, history, and culture. The document is intended to provide a comprehensive overview of various subjects, making it an ideal resource for research and educational purposes. However, due to its length, processing the entire document at once can be challenging for language models and other NLP tools. This is where text splitters like the RecursiveCharacterTextSplitter come into play. By dividing the text into smaller, more manageable chunks, we can improve the efficiency and accuracy of our analysis and processing tasks. The RecursiveCharacterTextSplitter is particularly useful because it not only splits the text based on character length but also recursively splits the resulting chunks if they exceed a specified character limit. This ensures that the chunks remain within the desired size range while preserving semantic coherence as much as possible.\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(text)\n",
    "docs = text_splitter.create_documents(chunks)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "--------------------------------------------------------------------------------\n",
      "def calculate_mean(numbers):\n",
      "--------------------------------------------------------------------------------\n",
      "return sum(numbers) / len(numbers)\n",
      "--------------------------------------------------------------------------------\n",
      "data = [10, 15, 20, 25, 30]\n",
      "--------------------------------------------------------------------------------\n",
      "mean_value = calculate_mean(data)\n",
      "--------------------------------------------------------------------------------\n",
      "print(f\"The mean value is: {mean_value}\")\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain import Language\n",
    "\n",
    "CODE_SNIPPET = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mean(numbers):\n",
    "   \n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "data = [10, 15, 20, 25, 30]\n",
    "mean_value = calculate_mean(data)\n",
    "print(f\"The mean value is: {mean_value}\")\n",
    "\"\"\"\n",
    "\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=10\n",
    ")\n",
    "code_docs = code_splitter.create_documents([CODE_SNIPPET])\n",
    "\n",
    "for doc in code_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80) \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "#     language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    "# )\n",
    "# python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "# python_docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Introduction</h1>\n",
      "--------------------------------------------------------------------------------\n",
      "<p>This is the first paragraph of the document. It provides an overview of the topic and sets the\n",
      "--------------------------------------------------------------------------------\n",
      "sets the context for the rest of the content.</p>\n",
      "--------------------------------------------------------------------------------\n",
      "<code>\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "--------------------------------------------------------------------------------\n",
      "greet(\"Alice\")\n",
      "</code>\n",
      "--------------------------------------------------------------------------------\n",
      "<p>The second paragraph delves deeper into the subject matter, offering more detailed information\n",
      "--------------------------------------------------------------------------------\n",
      "and insights.</p>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "mixed_text = \"\"\"\n",
    "<h1>Introduction</h1>\n",
    "<p>This is the first paragraph of the document. It provides an overview of the topic and sets the context for the rest of the content.</p>\n",
    "<code>\n",
    "def greet(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "greet(\"Alice\")\n",
    "</code>\n",
    "<p>The second paragraph delves deeper into the subject matter, offering more detailed information and insights.</p>\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "chunks = text_splitter.split_text(mixed_text)\n",
    "docs = text_splitter.create_documents(chunks)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a long text document.\n",
      "\n",
      "It has multiple sentences.\n",
      "\n",
      "The text should be split at sentence boundaries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "data = \"This is a long text document. It has multiple sentences. The text should be split at sentence boundaries.\"\n",
    "\n",
    "text_splitter = NLTKTextSplitter()\n",
    "\n",
    "docs =text_splitter.create_documents([data]) \n",
    "\n",
    "\n",
    "for doc in docs :\n",
    "    print(doc.page_content)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Introduction</h1>\n",
      "<p>This is the first paragraph.\n",
      "\n",
      "It provides an overview of the topic.\n",
      "\n",
      "The paragraph has multiple sentences.</p>\n",
      "<p>The second paragraph delves deeper into the subject matter.\n",
      "\n",
      "It offers more detailed information and insights.\n",
      "\n",
      "This paragraph also has multiple sentences.</p>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "mixed_text = \"\"\"\n",
    "<h1>Introduction</h1>\n",
    "<p>This is the first paragraph. It provides an overview of the topic. The paragraph has multiple sentences.</p>\n",
    "<p>The second paragraph delves deeper into the subject matter. It offers more detailed information and insights. This paragraph also has multiple sentences.</p>\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = NLTKTextSplitter()\n",
    "docs = text_splitter.create_documents([mixed_text])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have a tokenizer (like from the transformers library):  \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenTextSplitter\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load a tokenizer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Assuming you have a tokenizer (like from the transformers library):  \n",
    "\n",
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define the text\n",
    "text = \"This is a long text that needs to be split into smaller chunks for easier processing.\"\n",
    "\n",
    "# Create a token text splitter with a maximum of 10 tokens per chunk\n",
    "splitter = TokenTextSplitter(tokenizer=tokenizer, max_tokens=10)\n",
    "\n",
    "# Split the text\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text = \"This is a long text document that needs to be split into smaller chunks based on token count.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0, tokenizer=tokenizer)\n",
    "docs = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveTextSplitter\n",
    "\n",
    "# Define the text\n",
    "text = \"This is a long text that needs to be split into smaller chunks for easier processing. It has multiple sentences.\"\n",
    "\n",
    "# Create a recursive text splitter with a maximum length of 30 characters per chunk\n",
    "splitter = RecursiveTextSplitter(max_length=30)\n",
    "\n",
    "# Split the text\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter\n",
    "\n",
    "class CustomDelimiterTextSplitter(TextSplitter):\n",
    "    def __init__(self, delimiter):\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def split_text(self, text):\n",
    "        return text.split(self.delimiter)\n",
    "\n",
    "# Define the text\n",
    "text = \"Part1|Part2|Part3\"\n",
    "\n",
    "# Create a custom text splitter using '|' as the delimiter\n",
    "splitter = CustomDelimiterTextSplitter(delimiter='|')\n",
    "\n",
    "# Split the text\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Foo'),\n",
       " Document(page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some intro text about Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}),\n",
       " Document(page_content='Some text about the first subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}),\n",
       " Document(page_content='Some text about the second subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}),\n",
       " Document(page_content='Baz', metadata={'Header 1': 'Foo'}),\n",
       " Document(page_content='Some text about Baz', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}),\n",
       " Document(page_content='Some concluding text about Foo', metadata={'Header 1': 'Foo'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We see that Gödel first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (“This sentence is false”) and Berry’s paradox (“The least number not defined by an expression consisting of just fourteen English words”). Gödel then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='means that arithmetic truth and arithmetic provability are not co-extensive — whence the First Incompleteness Theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='This account of Gödel’s discovery was told to Hao Wang very much after the fact; but in Gödel’s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See Gödel 2003a and Gödel 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by Gödel by 1931. But he neither publicized nor published the result; the biases logicians', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to Gödel’s publication of that theorem.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='We now describe the proof of the two theorems, formulating Gödel’s results in Peano arithmetic. Gödel himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following Gödel’s notation.', metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "splits[80:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No two El Niño winters are the same, but many have temperature and precipitation trends in common.  \n",
      "Average conditions during an El Niño winter across the continental US.  \n",
      "One of the major reasons is the position of the jet stream, which often shifts south during an El Niño winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  \n",
      "Because the jet stream is essentially a river of air that storms flow through, they c\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "print(html_header_splits[1].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
