{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory in langchain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory stores our past intraction  , that is  conversation is stored in the memory \n",
    "\n",
    "you can use it usually \n",
    "you can used it in chains \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory in LangChain is a way to maintain state or context across interactions with a language model. This is essential for building applications like chatbots, where maintaining the context of a conversation is crucial. LangChain provides various memory structures to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Concepts\n",
    "Memory: Refers to the mechanism by which past interactions are stored and accessed to maintain context.\n",
    "\n",
    "ConversationHistoryMemory: Stores the entire conversation history.\n",
    "\n",
    "BufferMemory: Stores a fixed number of recent interactions.\n",
    "\n",
    "SummaryMemory: Summarizes the conversation at intervals.\n",
    "\n",
    "EntityMemory: Tracks specific entities mentioned during the conversation.\n",
    "\n",
    "VectorStoreMemory: Uses vector stores for efficient similarity search and context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.memory import ChatMessageHistory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config \n",
    "secrete_key = config('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  \n",
    "chat =ChatOpenAI(openai_api_key=secrete_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationHistoryMemory\n",
    "\n",
    "Explanation: This memory type stores the full conversation history, allowing the model to access and reference all previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationHistoryMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationHistoryMemory()\n",
    "\n",
    "# Initialize the language model with memory\n",
    "llm = OpenAI(model=\"text-davinci-003\", memory=memory)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = llm.generate(\"Hello, how are you?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = llm.generate(\"What's the weather like today?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = llm.generate(\"Can you remind me what I asked earlier?\")\n",
    "print(response3)\n",
    "\n",
    "# Print the memory buffer to see the stored conversation history\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hi how are you \n",
      "AI: hello iam fine  whats'up\n",
      "Human: what can you do for me?\n",
      "AI: everything related to solving everones problem \n"
     ]
    }
   ],
   "source": [
    "history=ChatMessageHistory() \n",
    "history.add_user_message(\"hi how are you \") \n",
    "history.add_ai_message(\"hello iam fine  whats'up\") \n",
    "\n",
    "history.add_user_message(\"what can you do for me?\") \n",
    "history.add_ai_message(\"everything related to solving everones problem \") \n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi how are you '), AIMessage(content=\"hello iam fine  whats'up\"), HumanMessage(content='what can you do for me?'), AIMessage(content='everything related to solving everones problem ')]\n"
     ]
    }
   ],
   "source": [
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory:\n",
    "BufferMemory\n",
    "\n",
    "Explanation: Stores only a fixed number of recent interactions, discarding older ones when the buffer is full.\n",
    "\n",
    "This is a simple in-memory buffer that stores the conversation history as a list of strings. It's useful for short-lived tasks or when you don't need to persist the memory beyond the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='hello'), AIMessage(content='hi whats up'), HumanMessage(content='how are you '), AIMessage(content='Iam AI i dont have fellings')])\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory   \n",
    "\n",
    "memory= ConversationBufferMemory() \n",
    "memory.save_context({\"input\":\"hello\"} , {\"output\":\"hi whats up\"})  \n",
    "memory.save_context({\"input\":\"how are you \"} , {\"output\":\"Iam AI i dont have fellings\"})\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hello\n",
      "AI: hi whats up\n",
      "Human: how are you \n",
      "AI: Iam AI i dont have fellings\n"
     ]
    }
   ],
   "source": [
    "# if you weant to get actual converation then use \n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: hello\\nAI: hi whats up\\nHuman: how are you \\nAI: Iam AI i dont have fellings'}\n"
     ]
    }
   ],
   "source": [
    "# if you want to get in term of dictionary format use \n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add a new interaction to the memory\n",
    "memory.save_context({\"input\": \"What is the capital of France?\"}, {\"output\": \"The capital of France is Paris.\"})\n",
    "\n",
    "# Retrieve the memory contents\n",
    "print(memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI: Hello! How are you today?\n",
      "Human: Who is MS Dhoni?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How are you today?\n",
      "MS Dhoni is a retired Indian cricketer who is widely regarded as one of the greatest wicketkeeper-batsmen in the history of the sport. He captained the Indian national cricket team and under his leadership, India won the ICC Cricket World Cup in 2011. Dhoni is known for his calm demeanor on the field and his ability to finish matches with his aggressive batting style. He is also an accomplished wicketkeeper, known for his lightning-fast stumpings.\n",
      "Human: Hi there\n",
      "AI: Hello! How are you today?\n",
      "Human: Who is MS Dhoni?\n",
      "AI: MS Dhoni is a retired Indian cricketer who is widely regarded as one of the greatest wicketkeeper-batsmen in the history of the sport. He captained the Indian national cricket team and under his leadership, India won the ICC Cricket World Cup in 2011. Dhoni is known for his calm demeanor on the field and his ability to finish matches with his aggressive batting style. He is also an accomplished wicketkeeper, known for his lightning-fast stumpings.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "\n",
    "# Initialize the conversation buffer memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with the language model and memory\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = conversation.predict(input=\"Hi there\")\n",
    "response2 = conversation.predict(input=\"Who is MS Dhoni?\")\n",
    "\n",
    "# Print the responses\n",
    "print(response1)\n",
    "print(response2)\n",
    "\n",
    "# Print the memory buffer to see the stored conversation history\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: good morning\n",
      "AI: good morning you too..\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory , ConversationBufferWindowMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# ConversationBufferWindowMemory \n",
    "\n",
    "memory =ConversationBufferWindowMemory(k=1)  \n",
    "\n",
    "memory.save_context({\"input\":\"hi\"} , {\"output\":\"what's up\"}) \n",
    "memory.save_context({\"input\":\"good morning\"} , {\"output\":\"good morning you too..\"})  \n",
    "\n",
    "print(memory.buffer)  # due to k val it will save latest memeory , if we take k=2 then it shows you latest 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi\n",
      "AI: Hello! How are you today?\n",
      "Human: who is the ms dhoni ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: hi\n",
      "AI: Hello! How are you today?\n",
      "Human: who is the ms dhoni ?\n",
      "AI: MS Dhoni is a former Indian cricketer and captain of the Indian national cricket team. He is considered one of the greatest finishers in limited-overs cricket and is known for his calm demeanor on the field. Dhoni is also one of the most successful captains in Indian cricket history, leading the team to victory in multiple ICC tournaments. He retired from international cricket in 2020.\n"
     ]
    }
   ],
   "source": [
    "# how we can use it with  chat model and wrt chains \n",
    "\n",
    "memory=ConversationBufferMemory(k=1) \n",
    "conversation= ConversationChain(llm=chat , memory=memory, verbose=True) \n",
    "\n",
    "conversation.predict(input=\"hi\") \n",
    "conversation.predict(input=\"who is the ms dhoni ?\")  \n",
    "print(memory.buffer)   # these conversation will take latest one  cause k=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationEntityMemory:\n",
    "\n",
    "what ever fact you given to ai entitity memeory remember it , and it extract info , and use it in future \n",
    "\n",
    "This memory type extracts entities (e.g., people, organizations, locations) from the conversation and stores them in a dictionary. It's useful for tasks that require tracking and referencing specific entities across multiple interactions. \n",
    "\n",
    "Explanation: Tracks specific entities mentioned during the conversation and maintains a record of their attributes and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI \n",
    "from langchain.memory import ConversationEntityMemory  \n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE \n",
    "# llm =OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='jay and Raj are working on hackathon project '), AIMessage(content=\"That's great to hear! Hackathons are a fantastic way to collaborate and work on exciting projects. Is there anything specific you need help with regarding the hackathon project Jay and Raj are working on?\"), HumanMessage(content='they are trying to add more complex memory structures to langchain '), AIMessage(content='That sounds like a fascinating project! Adding more complex memory structures to Langchain can definitely enhance its capabilities. If you have any specific questions or need assistance with implementing these memory structures, feel free to ask!'), HumanMessage(content='they are adding the key value store for entities mentioned so far in the conversation'), AIMessage(content=\"That's a great idea! Adding a key-value store for entities mentioned so far in the conversation can help manage and retrieve data efficiently. If you need any guidance on implementing this feature or have any specific questions about it, feel free to ask!\"), HumanMessage(content='what do you know about jay ?'), AIMessage(content=\"Jay is working on a hackathon project with Raj. If you have any specific questions or details you'd like to know about Jay, feel free to ask!\")]\n"
     ]
    }
   ],
   "source": [
    "memory =ConversationEntityMemory(llm=chat)  \n",
    "\n",
    "conversation =ConversationChain(llm=chat, memory=memory , prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE) \n",
    "\n",
    "conversation.predict(input= \"jay and Raj are working on hackathon project \") \n",
    "\n",
    "conversation.predict(input= \"they are trying to add more complex memory structures to langchain \") \n",
    "\n",
    "conversation.predict(input= \"they are adding the key value store for entities mentioned so far in the conversation\") \n",
    "\n",
    "conversation.predict(input= \"what do you know about jay ?\") \n",
    "\n",
    "\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to entity memeory then \n",
    "\n",
    "print(conversation.memory.entity_store.store) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory: \n",
    "\n",
    "ConversationSummaryBufferMemory is a memory type in LangChain that combines the features of BufferMemory and SummaryMemory\n",
    "\n",
    "It maintains a buffer of recent interactions and periodically summarizes the entire conversation history. This approach helps manage the context length more efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer: Keeps a fixed number of the most recent interactions.\n",
    "\n",
    "Summary: Summarizes the entire conversation history at regular intervals, which helps in reducing the context size while preserving important information from the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "\n",
    "memeory= ConversationSummaryBufferMemory(llm=chat , max_token_limit=40)\n",
    "\n",
    "conversation_with_summary_buff_memo= ConversationChain(llm=chat , memory=memory , verbose=True) \n",
    "\n",
    "conversation_with_summary_buff_memo.predict(input= \"why people are scared of ai \") \n",
    "\n",
    "conversation_with_summary_buff_memo.predict(input = \"what will be the impact of AI on humas in the future ?\")\n",
    " \n",
    " \n",
    "print(memeory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
      "\n",
      "\n",
      "\n",
      "Human: Why are people scared of AI?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
      "\n",
      "System: The human asks why people are scared of AI. The AI explains that people fear the unknown, losing control, job loss, privacy invasion, and ethical implications. Educating ourselves about AI is important to alleviate these fears.\n",
      "\n",
      "Human: What will be the impact of AI on humans in the future?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'history': 'System: The human asks why people are scared of AI. The AI explains that people fear the unknown, losing control, job loss, privacy invasion, and ethical implications. Educating ourselves about AI is important to alleviate these fears. The human asks what will be the impact of AI on humans in the future. The AI responds that AI will continue to have a significant impact, revolutionizing industries, improving efficiency, and enhancing quality of life. However, there are concerns about job displacement, ethical dilemmas, and potential misuse. Society must navigate these challenges responsibly and ethically.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the memory with the correct configuration\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=40)\n",
    "\n",
    "# Define a custom prompt template if needed\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\n\"\"\"\n",
    "             \"{history}\\n\\n\"\n",
    "             \"Human: {input}\\n\"\n",
    "             \"AI:\"\n",
    ")\n",
    "\n",
    "# Initialize the conversation chain with the custom prompt and memory\n",
    "conversation_with_summary_buff_memo = ConversationChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt_template,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Perform the conversation\n",
    "conversation_with_summary_buff_memo.predict(input=\"Why are people scared of AI?\")\n",
    "conversation_with_summary_buff_memo.predict(input=\"What will be the impact of AI on humans in the future?\")\n",
    "\n",
    "# Print the memory variables\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationSummaryBuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "memory = ConversationEntityMemory()\n",
    "\n",
    "# Add a new interaction to the memory\n",
    "memory.save_context({\"input\": \"What is the capital of France?\"}, {\"output\": \"The capital of France is Paris.\"})\n",
    "\n",
    "# Retrieve the memory contents\n",
    "print(memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import EntityMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the memory\n",
    "memory = EntityMemory()\n",
    "\n",
    "# Initialize the language model with memory\n",
    "llm = OpenAI(model=\"text-davinci-003\", memory=memory)\n",
    "\n",
    "# Simulate a conversation involving entities\n",
    "response1 = llm.generate(\"My dog's name is Max.\")\n",
    "print(response1)\n",
    "\n",
    "response2 = llm.generate(\"Max loves playing fetch.\")\n",
    "print(response2)\n",
    "\n",
    "# Memory helps maintain entity context\n",
    "response3 = llm.generate(\"What's my dog's name?\")\n",
    "print(response3)\n",
    "\n",
    "# Print the memory buffer to see the stored conversation history\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationSummaryMemory:\n",
    "\n",
    "This memory type summarizes the conversation history into a condensed form, which can be useful for maintaining context in long conversations or when memory resources are limited.\n",
    "\n",
    "Explanation: Summarizes the conversation at regular intervals, storing summaries instead of the full conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import SummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the memory with a summary frequency\n",
    "memory = SummaryMemory(summary_frequency=3)\n",
    "\n",
    "# Initialize the language model with memory\n",
    "llm = OpenAI(model=\"text-davinci-003\", memory=memory)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = llm.generate(\"Hi! What's your name?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = llm.generate(\"Can you tell me a fun fact?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = llm.generate(\"What's the weather like?\")\n",
    "print(response3)\n",
    "\n",
    "# After three interactions, the memory summarizes the conversation\n",
    "response4 = llm.generate(\"Remind me of the fun fact you mentioned.\")\n",
    "print(response4)\n",
    "\n",
    "# Print the memory buffer to see the stored conversation history\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=your_language_model)\n",
    "\n",
    "# Add a new interaction to the memory\n",
    "memory.save_context({\"input\": \"What is the capital of France?\"}, {\"output\": \"The capital of France is Paris.\"})\n",
    "\n",
    "# Retrieve the memory summary\n",
    "print(memory.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorStoreRetrieverMemory:\n",
    "\n",
    "you can load documnet here \n",
    "\n",
    "This memory type stores the conversation history in a vector store (e.g., Chroma, FAISS, or Weaviate), which allows for efficient retrieval of relevant information based on vector similarity. It's useful for tasks that require searching and retrieving specific information from a large corpus of data.\n",
    "\n",
    "Explanation: Uses vector stores (like FAISS or Pinecone) to store conversation history or knowledge in a way that allows for efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain.memory import VectorStoreRetrieverMemory \n",
    "from langchain_community.vectorstores import Chroma  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi my name is ironman\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: my favorite game is football \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: my favorite game is football \n",
      "response: That's great to hear! Football is a popular sport enjoyed by many around the world. Do you have a favorite team or player?\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: my favorite coutry in france \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: my favorite game is football \n",
      "response: That's great to hear! Football is a popular sport enjoyed by many around the world. Do you have a favorite team or player?\n",
      "input: my favorite coutry in france \n",
      "response: France is a beautiful country known for its rich history, culture, and cuisine. From the iconic Eiffel Tower in Paris to the stunning beaches of the French Riviera, there is so much to explore and enjoy in France. Do you have a specific city or region in France that you particularly like?\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human:  what is my favorite game ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I do not have access to that information. Can you please remind me what your favorite game is?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_func = OpenAIEmbeddings(openai_api_key= secrete_key)  \n",
    "\n",
    "db=Chroma(embedding_function=embedding_func , persist_directory=\"./chro1_db\")  \n",
    "\n",
    "\n",
    "retriever =db.as_retriever()\n",
    "\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever) \n",
    "\n",
    "conversation=ConversationChain(llm=chat , memory=memory , verbose=True)\n",
    "\n",
    "conversation.predict(input=\"hi my name is ironman\" ) \n",
    "conversation.predict(input=\"my favorite game is football \" ) \n",
    "# conversation.predict(input=\"my favorite food is burger\") \n",
    "conversation.predict(input=\"my favorite coutry in france \")  \n",
    "conversation.predict(input=\" what is my favorite game ?\" ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi my name is ironman\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: my favorite game is football\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: my favorite game is football\n",
      "response: That's great to hear! Football is a popular sport played by two teams of eleven players each, using an oval-shaped ball. The objective is to score points by advancing the ball into the opposing team's end zone. Are you a fan of a specific football team or do you enjoy playing football yourself?\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: my favorite food is burger\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: my favorite game is football\n",
      "response: That's great to hear! Football is a popular sport played by two teams of eleven players each, using an oval-shaped ball. The objective is to score points by advancing the ball into the opposing team's end zone. Are you a fan of a specific football team or do you enjoy playing football yourself?\n",
      "input: my favorite food is burger\n",
      "response: Burgers are a delicious choice! They typically consist of a grilled or fried patty made from ground beef or other meat, served in a sliced bun with various toppings like lettuce, tomato, cheese, onions, pickles, and condiments such as ketchup and mustard. Do you have a favorite type of burger or a specific place you like to get them from?\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: my favorite country is France\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: my favorite game is football\n",
      "response: That's great to hear! Football is a popular sport played by two teams of eleven players each, using an oval-shaped ball. The objective is to score points by advancing the ball into the opposing team's end zone. Are you a fan of a specific football team or do you enjoy playing football yourself?\n",
      "input: my favorite food is burger\n",
      "response: Burgers are a delicious choice! They typically consist of a grilled or fried patty made from ground beef or other meat, served in a sliced bun with various toppings like lettuce, tomato, cheese, onions, pickles, and condiments such as ketchup and mustard. Do you have a favorite type of burger or a specific place you like to get them from?\n",
      "input: my favorite country is France\n",
      "response: France is a beautiful country known for its rich history, culture, and cuisine. It is located in Western Europe and is famous for landmarks such as the Eiffel Tower, Louvre Museum, and Palace of Versailles. French cuisine is also highly regarded worldwide, with dishes like croissants, escargot, and coq au vin being popular choices. Have you ever visited France or do you dream of traveling there someday?\n",
      "input: hi my name is ironman\n",
      "response: Hello Ironman! It's nice to meet you. How can I assist you today?\n",
      "Human: what is my favorite game?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is my favorite game?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Print the memory variables\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(memory\u001b[38;5;241m.\u001b[39mload_memory_variables({}))\n",
      "File \u001b[1;32mc:\\Users\\Jayraj\\anaconda3\\Lib\\site-packages\\langchain\\memory\\vectorstore.py:56\u001b[0m, in \u001b[0;36mVectorStoreRetrieverMemory.load_memory_variables\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_memory_variables\u001b[39m(\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[0;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[List[Document], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return history buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     input_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prompt_input_key(inputs)\n\u001b[0;32m     57\u001b[0m     query \u001b[38;5;241m=\u001b[39m inputs[input_key]\n\u001b[0;32m     58\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[1;32mc:\\Users\\Jayraj\\anaconda3\\Lib\\site-packages\\langchain\\memory\\vectorstore.py:39\u001b[0m, in \u001b[0;36mVectorStoreRetrieverMemory._get_prompt_input_key\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the input key for the prompt.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_prompt_input_key(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_variables)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key\n",
      "File \u001b[1;32mc:\\Users\\Jayraj\\anaconda3\\Lib\\site-packages\\langchain\\memory\\utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[1;34m(inputs, memory_variables)\u001b[0m\n\u001b[0;32m     17\u001b[0m prompt_input_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(inputs)\u001b[38;5;241m.\u001b[39mdifference(memory_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne input key expected got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_input_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: One input key expected got []"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# Initialize the embedding function\n",
    "embedding_func = OpenAIEmbeddings(openai_api_key=secrete_key)\n",
    "\n",
    "# Create and persist the Chroma vector store\n",
    "db = Chroma(embedding_function=embedding_func, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Convert the vector store to a retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Initialize the memory with the retriever\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Create the conversation chain with memory\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Perform the conversation\n",
    "conversation.predict(input=\"hi my name is ironman\")\n",
    "conversation.predict(input=\"my favorite game is football\")\n",
    "conversation.predict(input=\"my favorite food is burger\")\n",
    "conversation.predict(input=\"my favorite country is France\")\n",
    "conversation.predict(input=\"what is my favorite game?\")\n",
    "\n",
    "# Print the memory variables\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create a vector store\n",
    "vector_store = Chroma.from_texts([\"What is the capital of France?\"], embeddings)\n",
    "\n",
    "# Create the memory instance\n",
    "memory = VectorStoreRetrieverMemory(vector_store)\n",
    "\n",
    "# Add a new interaction to the memory\n",
    "memory.save_context({\"input\": \"What is the capital of France?\"}, {\"output\": \"The capital of France is Paris.\"})\n",
    "\n",
    "# Retrieve relevant information from the memory\n",
    "print(memory.get_relevant_info(\"capital\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = FAISS()\n",
    "\n",
    "# Initialize the memory with the vector store\n",
    "memory = VectorStoreMemory(vector_store=vector_store)\n",
    "\n",
    "# Initialize the language model with memory\n",
    "llm = OpenAI(model=\"text-davinci-003\", memory=memory)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = llm.generate(\"I love Italian food.\")\n",
    "print(response1)\n",
    "\n",
    "response2 = llm.generate(\"What's your favorite Italian dish?\")\n",
    "print(response2)\n",
    "\n",
    "# Memory uses vector similarity for context\n",
    "response3 = llm.generate(\"Do you know any good Italian restaurants?\")\n",
    "print(response3)\n",
    "\n",
    "# Print the memory buffer to see the stored conversation history\n",
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CombinedMemory:\n",
    "\n",
    "This memory type allows you to combine multiple memory implementations into a single memory instance. It's useful when you need to leverage different memory types for different purposes within the same application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, CombinedMemory\n",
    "\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "combined_memory = CombinedMemory(memories=[buffer_memory])\n",
    "\n",
    "# Add a new interaction to the memory\n",
    "combined_memory.save_context({\"input\": \"What is the capital of France?\"}, {\"output\": \"The capital of France is Paris.\"})\n",
    "\n",
    "# Retrieve the memory contents\n",
    "print(combined_memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Application\n",
    "By using these memory types, you can build applications that need to remember previous interactions and maintain context. For instance:\n",
    "\n",
    "Customer Support: Agents can remember previous interactions with a customer, providing more personalized and efficient support.\n",
    "\n",
    "Interactive Fiction: Games can use memory to keep track of player choices and events, creating a dynamic and responsive narrative.\n",
    "\n",
    "Education: Educational tools can remember student progress and interactions, offering tailored guidance and feedback.\n",
    "\n",
    "\n",
    "By leveraging different types of memory, LangChain enables developers to create more sophisticated and context-aware applications with large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
